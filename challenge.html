<!DOCTYPE HTML>
<!--
	TXT by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html xmlns="http://www.w3.org/1999/html">

<head>
    <title>MEGC 2026</title>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport" />
    <link href="assets/css/main.css" rel="stylesheet" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body class="is-preload">
    <div id="page-wrapper">

        <!-- Header -->
        <header id="header">
            <div class="logo container">
                <div>
                    <h1><a href="index.html" id="logo">MEGC @ FG 2026</a></h1>
                </div>
            </div>
        </header>

        <!-- Nav -->
        <nav id="nav">
            <ul>
                <!-------------BEGIN SIDE MENU--------------->
                <li><a href="index.html">Home</a></li>
                <!-- Workshop MENU-->
                <li><a href="workshop.html">FME Workshop</a>
                    <ul>
                        <li><a href="workshop.html#agenda">Agenda</a></li>
                        <li><a href="workshop.html#submission">Submission</a></li>
                    </ul>
                </li>
                <!-- Challenge MENU -->
                <li class="current"><a href="challenge.html">MEGC</a>
                    <ul>
                        <li><a href="challenge.html#news">News</a></li>
                        <li><a href="challenge.html#dates">Important Dates</a></li>
                    </ul>
                </li>
                <!-- <li><a href="program.html">Program</a></li> -->
                <li><a href="organisers.html">Organisers</a></li>
                <li><a href="review.html">Continuity</a></li>
                <!-------------END OF SIDE MENU--------------->
            </ul>
        </nav>

        <!-- Main -->
        <section id="main">
            <div class="container">

                <!-- Pre-amble section -->
               <section>
                    <header class="main">
                        <h2>Micro-Expression Grand Challenge (MEGC) 2026</h2>
                    </header>
                   <p>
                    Facial micro-expressions (MEs) are spontaneous, fleeting facial movements (less than 500ms) that reveal suppressed emotions, making them critical for high-stakes analysis but notoriously difficult to detect due to data scarcity and annotation challenges. MEGC 2026 aims to bridge this gap by leveraging the emerging capabilities of <strong>Vision-Language Models (VLMs)</strong> and <strong>Multimodal Large Language Models (LLMs)</strong>. While these models excel in general reasoning, their ability to interpret the subtle, fine-grained cues of micro-expressions remains an open research question that this competition seeks to answer through novel benchmarking tasks.
                </p>
                <p>
                    By introducing <strong>Micro-Expression Short-Video (ME-VQA)</strong> and the pioneering <strong>Long-Video Question Answering (ME-LVQA)</strong> tracks, the competition pushes boundaries from simple recognition to interpretable natural language understanding. Advancing this technology will have transformative impacts across <strong>healthcare</strong> (for mental state assessment), <strong>human–machine interaction</strong> (creating empathetic systems), and <strong>security</strong> (enhancing risk assessment), ultimately deepening our scientific understanding of nonverbal human communication.
                </p>
                    <p>
                        In detail, MEGC 2026 introduces:
                    </p>
                    <ul>
                        <li><strong>Task 1 - ME-VQA:</strong> a continuation of the ME-VQA task (MEGC 2025) for benchmarking short-clip subtle-emotion reasoning.</li>
                        <li><strong>Task 2 - ME-LVQA:</strong> a new ME-LVQA task <em>Micro-Expression Long-Video Question Answering</em>, requiring reasoning and description across long videos.</li>
                    </ul>

                </section>

                <!-- News Section -->
                <section>
                    <a name="news"></a>
                    <h2 class="major"><span>News</span></h2>
                    <ul>
                        <li><b>22/12/2025: </b>Website made live</li>
                    </ul>
                </section>

                <!-- Important Dates Section -->
                <section>
                <a name="dates"></a>
                <h2 class="major"><span>Important Dates</span></h2>
                <p>The competition schedule is aligned with the FG 2026 timeline:</p>
                <ul>
                    <li><b>Competition Period: </b>
                        23rd December 2025 – 9th March 2026</li>
                    <li><b>Training Set Release: </b>
                        23rd December 2025</li>
                    <li><b>Baseline Codes Release: </b>
                        15th January 2026</li>
                    <li><b>Test Set Release: </b>
                        28th February 2026</li>
                    <li><b>Competition End Date: </b>
                        9th March 2026 <font color="red">(FIRM)</font></li>
                    <li><b>Winner Paper Submissions and Code Verification: </b>
                        23rd March 2026</li>
                    <li><b>Notification of Acceptance: </b>
                        2nd April 2026 (FG 2026 Round 2 Notification)</li>
                    <li><b>Camera-Ready Deadline: </b>
                        21st April 2026 (FG 2026 Camera-Ready)</li>
                    <li><b>Challenge Date: </b>
                        25th or 29th May 2026 (during FG 2026)</li>
                </ul>
            </section>
 <section>
                    <a name="tasks"></a>
                    <h2 class="major"><span>Tasks and Application Scenarios</span></h2>

                    <h3>Task 1: ME-VQA (Micro-Expression Visual Question Answering)</h3>
                    <p>This task extends MEGC 2025. Participants receive short video clips containing a single ME and must answer natural-language questions such as:</p>
                    <ul>
                        <li>"What fine-grained emotion is expressed?"</li>
                        <li>"What action units are shown on the face?"</li>
                        <li>"Please provide an analysis and the reasoning process of the expression."</li>
                        <li>...</li>
                    </ul>
 <p>  </p>
                    <h3>Task 2: ME-LVQA (Micro-Expression Long-Video Question Answering)</h3>
                    <p>This is a new FG 2026 task. Participants are given <em>long videos</em>. Each video may contain multiple or zero MEs. Participants must describe and explain all ME-related information contained in these long videos. Questions may include:</p>
                    <ul>
                        <li>"How many micro-expressions occur in this video?"</li>
                        <li>"What emotions are expressed?"</li>
                        <li>"Explain which facial actions occur in this video and why they indicate these emotions."</li>
                        <li>...</li>
                    </ul>
                </section>

                <section>
                    <a name="data"></a>
                    <h2 class="major"><span>Data</span></h2>
                    <p>The training data for the competition is recommended from existing ME datasets that are publicly available. These recommendations are split based on the tasks previously described. In addition, we include unseen test data that is used for the final results calculation for both tasks.</p>

                    <h3>Task 1 - ME-VQA - Recommended Training Data</h3>
                    <ul>
                        <li><strong>Curated ME VQA dataset:</strong> Improved from the MEGC2019 composite dataset with clips from SAMM, CASME II, and SMIC by adding QA pairs.</li>

                        <li><strong>SAMM</strong> (159 ME clips at 100 fps):
                            <ul>
                            <li> To download the dataset, please visit: <a
                                    href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php</a>.
                                Download and fill in the license agreement form, email to <a href="M.Yap@mmu.ac.uk">M.Yap@mmu.ac.uk</a>
                                with email subject: SAMM videos.
                            </li>
                            <li>
                                Reference: Dvison, A. K., Lansley, C., Costen, N., Tan, K., & Yap, M. H. (2016). SAMM: A spontaneous micro facial movement
                                dataset. IEEE Transactions on Affective Computing, 9(1), 116-129.
                            </li>
                        </uL>
                        </li>

                        <li><strong>CASME II</strong> (247 ME clips at 200 fps):
                            <ul>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e3">http://casme.psych.ac.cn/casme/e3</a>. Download and fill in the license agreement form, submit throuth the website.
                            >.
                            </li>
                            <li>
                                Reference: Yan, W. J., Li, X., Wang, S. J., Zhao, G., Liu, Y. J., Chen, Y. H., & Fu, X. (2014). CASME II: An improved
                                spontaneous micro-expression database and the baseline evaluation. PloS one, 9(1), e86041.
                            </li>
                        </ul>
                        </li>

                        <li><strong>CAS(ME)<sup>3</sup></strong> (1300 long videos at 30 fps):
                            <ul>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e4">http://casme.psych.ac.cn/casme/e4</a>.
                                Download and fill in the license agreement form, submit throuth the website.
                            </li>
                            <li>
                                Reference: Li, J., Dong, Z., Lu, S., Wang, S. J., Yan, W. J., Ma, Y., ... & Fu, X. (2022). CAS (ME)<sup>3</sup>: A third
                                generation facial spontaneous micro-expression database with depth information and high ecological validity. IEEE Transactions
                                on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 2782-2800, doi: 10.1109/TPAMI.2022.3174895.
                            </li>

                        </ul>
                        </li>

                        <li><strong>SMIC-E-long</strong> (162 long videos at 100 fps):
                             <ul>
                            <li> To download the dataset, please visit: <a href="https://www.oulu.fi/cmvs/node/41319">https://www.oulu.fi/cmvs/node/41319</a>.
                                Download and fill in the license agreement form (please indicate which version/subset
                                you need), email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Tran, T. K., Vo, Q. N., Hong, X., Li, X., & Zhao, G. (2021). Micro-expression
                                spotting: A new benchmark. Neurocomputing, 443, 356-368.
                            </li>
                        </ul>
                        </li>

                        <li><strong>4DME</strong> (1068 ME clips at 60 fps):
                           <ul>
                            <li> To download the dataset, please visit: <a
                                    href="https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis">https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis</a>.
                                Download and fill in the license agreement form , email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Li, X., Cheng, S., Li, Y., Behzad, M., Shen, J., Zafeiriou, S., ... & Zhao, G. (2022). 4DME: A spontaneous
                                4D micro-expression dataset with multimodalities. IEEE Transactions on Affective Computing.
                            </li>
                        </ul>
                        </li>
                    </ul>
 <p>  </p>
                    <h3>Task 2 - ME-LVQA - Recommended Training Data</h3>
                    <ul>
                        <li><strong>SAMM Long Videos</strong> (147 long videos at 200 fps):
                            <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php</a>.
                                Download and fill in the license agreement form, email to <a href="M.Yap@mmu.ac.uk">M.Yap@mmu.ac.uk</a>
                                with email subject: SAMM long videos.
                            </li>
                            <li>
                                Reference: Yap, C. H., Kendrick, C., & Yap, M. H. (2020, November). SAMM long videos: A
                                spontaneous facial micro-and macro-expressions dataset. In 2020 15th IEEE International
                                Conference on Automatic Face and Gesture Recognition (FG 2020) (pp. 771-776). IEEE.
                            </li>
                        </uL>
                        </li>
                        <li><strong>CAS(ME)<sup>2</sup></strong> (97 long videos at 30 fps):
                            <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e3">http://casme.psych.ac.cn/casme/e3</a>.  Download and fill in the license agreement form, submit throuth the website. .
                            </li>
                            <li>
                                Reference: Qu, F., Wang, S. J., Yan, W. J., Li, H., Wu, S., & Fu, X. (2017). CAS (ME) $^
                                2$: a database for spontaneous macro-expression and micro-expression spotting and
                                recognition. IEEE Transactions on Affective Computing, 9(4), 424-436.
                            </li>

                        </uL>
</li>
                        <li><strong>CAS(ME)<sup>3</sup></strong> (1300 long videos at 30 fps):
                           <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://casme.psych.ac.cn/casme/e4">http://casme.psych.ac.cn/casme/e4</a>.
                                Download and fill in the license agreement form, submit throuth the website.
                            </li>
                            <li>
                                Reference: Li, J., Dong, Z., Lu, S., Wang, S. J., Yan, W. J., Ma, Y., ... & Fu, X. (2022). CAS (ME)<sup>3</sup>: A third generation facial spontaneous micro-expression database with depth information and high ecological validity. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 3, pp. 2782-2800, doi: 10.1109/TPAMI.2022.3174895..
                            </li>

                        </uL> </li>
                        <li><strong>SMIC-E-long</strong> (162 long videos at 100 fps):
                            <uL>
                            <li> To download the dataset, please visit: <a href="https://www.oulu.fi/cmvs/node/41319">https://www.oulu.fi/cmvs/node/41319</a>.
                                Download and fill in the license agreement form (please indicate which version/subset
                                you need), email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Tran, T. K., Vo, Q. N., Hong, X., Li, X., & Zhao, G. (2021). Micro-expression
                                spotting: A new benchmark. Neurocomputing, 443, 356-368.
                            </li>
                        </uL></li>
                        <li><strong>4DME</strong> (270 long videos at 60 fps):
                           <uL>
                            <li> To download the dataset, please visit: <a
                                    href="https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis">https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis</a>.
                                Download and fill in the license agreement form , email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Li, X., Cheng, S., Li, Y., Behzad, M., Shen, J., Zafeiriou, S., ... & Zhao, G. (2022). 4DME: A spontaneous 4D micro-expression dataset with multimodalities. IEEE Transactions on Affective Computing.
                            </li>
                        </ul></li>

                        <li><strong>ME-LVQA:</strong> Will be prepared and released when we release the competition baseline.</li>
                    </ul>
 <p>  </p>
                    <h3>Unseen Test Data</h3>
                    <p>
                        The unseen test set for the VQA task contains 24 ME clips, including 7 clips from SAMM (SAMM Challenge dataset) and 17 clips from different videos in CAS(ME)<sup>3</sup>.
                        <br>
                        The unseen test set for the LVQA task will contain 30 long videos, including 10 long videos from SAMM and 20 clips cropped from different videos in CAS(ME)<sup>3</sup>.
                    </p>
                    <p>To obtain the test sets, participants will be required to download and fill in the license agreement forms for the SAMM Challenge dataset and CAS(ME)<sup>3</sup>, and upload them to the organisers through a link (to be provided when the test sets are released).</p>
                </section>


 <section>
                    <a name="evaluation"></a>
                    <h2 class="major"><span>Evaluation Metrics</span></h2>
                    <p>To evaluate the performance of both our ME-VQA and ME-LVQA, we report metrics for both emotion classification and the overall language answer quality. The final ranking of the challenge participants will be based on the average (Avg) of all the overall metrics.</p>

                    <h3>Emotion Classification</h3>
                    <p>For both coarse- and fine-grained emotion classification, we use <strong>Unweighted F1 Score (UF1)</strong> and <strong>Unweighted Average Recall (UAR)</strong> to ensure balanced evaluation across classes.</p>
                    <ul>
                        <li><strong>Coarse classes:</strong> positive, negative, surprise.</li>
                        <li><strong>Fine-grained classes:</strong> happiness, surprise, fear, disgust, anger, sadness.</li>
                    </ul>
 <p>  </p>
                    <h3>Generated Text Quality</h3>
                    <p>For all VQA answers, we report <strong>BLEU</strong> and <strong>ROUGE-1</strong> to assess the quality of generated text.</p>

                    <p><strong>BLEU</strong> evaluates n-gram precision between predicted and reference answers as:</p>
                    <p>
                        $$ \mathrm{BLEU} = \exp \left( \min\left(1 - \frac{r}{c}, 0\right) + \sum_{n=1}^{N} w_n \log p_n \right) $$
                    </p>

                    <p><strong>ROUGE-1</strong> score is defined as the recall of unigram overlaps between the candidate answer <em>C</em> and the reference answer <em>R</em>:</p>
                    <p>
                        $$ \mathrm{ROUGE\text{-}1} = \frac{\sum_{w \in V} \min\big(\mathrm{N}_p(w),\ \mathrm{N}_r(w)\big)}{\sum_{w \in V} \mathrm{N}_r(w)} $$
                    </p>

                </section>

                <section>
                <a name="baselines"></a>
                <h2 class="major"><span>Baselines</span></h2>
                <p>
                    We will release a baseline paper describing the core methodology and evaluation strategy, and example submissions to clarify output formats. To support participants and ensure fair comparison, we will provide a comprehensive suite of baseline methods, reference implementations, and evaluation tools for both ME-VQA and ME-LVQA.
                </p>
                <ul>
                    <li>
                        <b>For ME-VQA:</b> We will release the baseline models used in MEGC 2025. These baselines demonstrate how short video clips can be paired with natural-language questions and offer strong reference points for zero-shot and fine-tuned performance.
                    </li>
                    <li>
                        <b>For ME-LVQA:</b> We will introduce a new baseline pipeline for long video question answering. This pipeline provides participants with an end-to-end reference workflow, illustrating how natural-language reasoning can be integrated.
                    </li>
                </ul>
                <p>
                    Finally, the competition will be hosted on the <b>Codabench</b> platform, which provides a transparent and secure leaderboard, reproducible evaluations, and strict submission management. Codabench ensures fairness by preventing label leakage, enforcing submission limits, and enabling consistent scoring across all participants.
                </p>
            </section>

                <!-- Submission Section -->
                 <section>
                    <a name="submission"></a>
                    <h2 class="major"><span>Submission</span></h2>

                    <h3>Competition Submission</h3>
                    <p>The MEGC 2026 competition will use a dedicated Codabench-based competition website. </p>
                    <ul>
                       <li><b>Challenge submission platform for ME-VQA task: TBC</b></li>
                    <li><b>Challenge submission platform for ME-LVQA task: TBC</b></li>
                        <li><strong>Evaluation result file (.json)</strong> : This file must include, for every tested video, the
dataset name, video ID, and the corresponding predicted outputs. The Codabench
platform will parse and evaluate these results, displaying metric-specific scores and
leaderboard rankings in real time. A daily submission limit will be imposed to prevent
leaderboard overfitting.</li>
                    </ul>

                    <h3>Rules</h3>
                    <ul>
                        <li><strong>Use of external data:</strong> Permitted if clearly declared.</li>
                        <li><strong>Use of pretrained models:</strong> Permitted (e.g., VLMs, LLMs) if publicly available and disclosed.</li>
                        <li><strong>Prohibition of test-set leakage:</strong> Teams must not attempt to access or reverse engineer test labels.</li>
                        <li><strong>Submission limits:</strong> Maximum of 3 submissions per day to Codabench.</li>
                        <li><strong>Mandatory technical report:</strong> Required for eligibility.</li>
                    </ul>

                    <h3>Cheating Prevention</h3>
                    <p>Hidden test sets, rate-limited submissions, metadata monitoring, cross-checks for label leakage, and manual verification for winning teams will be employed.</p>

                    <h3>Paper Submission</h3>
                    <ul>
                    <li>Paper submissions will follow the FG 2026 formatting (long paper format, 8 pages + refs) and use the CMT system. Accepted papers will be included in the FG 2026 challenge proceedings.</li>
                    <li>For all other required files besides the paper, please submit in a
                                single zip file and upload to the submission system as supplementary material. It is compulsory to include:
                                <ul>
                                    <li>GitHub repository URL containing codes of your implemented method, and all other
                                        relevant files such as feature/parameter data.</li>
                                    <li>CSV files reporting the results.</li>
                                </ul>
                                The organizers have the right to reject any submissions that: 1) are not accompanied by a paper,
                                2) did not share the code repository and reported results for verification purposes.
                            </li>
                     <li><strong>Submission Link:</strong>TBC</li>
                         </ul>
                </section>

                <section>
                    <p>For inquiries, please get in touch via the email addresses listed below.</p>
                </section>



            </div>
        </section>
        <footer id="footer">
            <!-- Copyright -->
            <div id="copyright">
                <ul class="menu">
                    <li>GET IN TOUCH: a.davison@mmu.ac.uk | x.fan@mmu.ac.uk | j.see@hw.ac.uk | lijt@psych.ac.cn</li>
                    <li>&copy; MEGC and FME Workshop 2026. All rights reserved</li>
                    <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </footer>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.dropotron.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>

</html>