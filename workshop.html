<!DOCTYPE HTML>
<!--
	TXT by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>FME Workshop 2026</title>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport" />
    <link href="assets/css/main.css" rel="stylesheet" />
</head>

<body class="is-preload">
    <div id="page-wrapper">

        <!-- Header -->
        <header id="header">
            <div class="logo container">
                <div>
                    <h1><a href="index.html" id="logo">FME Workshop @ FG 2026 </a></h1>
                </div>
            </div>
        </header>

        <!-- Nav -->
        <nav id="nav">
            <ul>
                <!-------------BEGIN SIDE MENU--------------->
                <li><a href="index.html">Home</a></li>
                <!-- Workshop MENU-->
                <li class="current"><a href="workshop.html">FME Workshop</a>
                    <ul>
                        <li><a href="workshop.html#agenda">Agenda</a></li>
                        <li><a href="workshop.html#submission">Submission</a></li>
                    </ul>
                </li>
                <!-- Challenge MENU -->
                <li><a href="challenge.html">MEGC</a>
                    <ul>
                        <li><a href="challenge.html#news">News</a></li>
                        <li><a href="challenge.html#dates">Important Dates</a></li>
                    </ul>
                </li>
                <!-- <li><a href="program.html">Program</a></li> -->
                <li><a href="organisers.html">Organisers</a></li>
                <li><a href="review.html">Continuity</a></li>
                <!-------------END OF SIDE MENU--------------->
            </ul>
        </nav>

        <!-- Main -->
        <section id="main">
            <div class="container">

                <!-- Content -->
                <!-- Pre-amble Section -->
                <section>
    <header class="main">
        <h2 align="center">Facial Micro-Expression (FME) Workshop 2026</h2>
        <h2 align="center">Pushing Boundaries in Temporal and Spatial Subtle Movement Analysis</h2>
        <p align="center">This workshop focuses on advancing the study of facial micro-expressions (FME) in computational analysis, spanning across interdisciplinary fields and incorporating the latest techniques in machine learning, multimodal analysis, and more.</p>
    </header>
</section>

<!-- Workshop Focus / Call for Papers Section (replacing the old Agenda section) -->
<section>
  <header class="main">
    <h2 class="major"><span> Intro</span></h2>
    <!-- Keep the anchor name="agenda" to preserve existing nav links -->
    <a name="agenda"></a>
  </header>

  <p>
    Facial micro-expressions (MEs) are brief, involuntary facial movements that occur when individuals experience emotions
    they attempt to suppress, often in high-stakes scenarios. With durations typically below 500 ms, MEs provide unique
    cues for hidden or subtle affect, yet remain difficult to spot and recognize computationally due to scarce labeled
    data, annotation ambiguity, and the need for fine-grained spatiotemporal modeling.
  </p>

  <p>
    This workshop aims to bring together researchers across computer vision, multimedia, and affective computing to
    advance temporal and spatial subtle movement analysis. We particularly encourage work that leverages modern learning
    paradigms (e.g., self-supervised learning, multimodal learning, and foundation models) while addressing challenges in
    data, evaluation, and robustness.
  </p>

  <h3>Focus</h3>
  <ul>
    <li><strong>Micro-expression spotting & recognition</strong> under limited supervision and real-world variability</li>
    <li><strong>Fine-grained spatiotemporal modeling</strong> of subtle facial motion in videos</li>
    <li><strong>Learning with scarce / noisy / inconsistent labels</strong> and annotation uncertainty</li>
    <li><strong>Multimodal ME analysis</strong> beyond RGB (e.g., NIR/IR, depth, audio, physiological signals)</li>
    <li><strong>Foundation models for subtle affect</strong> (VLMs / multimodal LLMs), including prompt-based and zero-shot setups</li>
    <li><strong>Datasets, benchmarks, and protocols</strong> that improve reproducibility and fair comparison</li>
  </ul>
</section>

    <section>
  <header class="main">
    <h2 class="major"><span>Call for Papers</span></h2>
    <!-- Keep the anchor name="agenda" to preserve existing nav links -->
    <a name="agenda"></a>
  </header>
  <h3>Topics of Interest:</h3>
  <p>
    We invite original, unpublished submissions (including position papers and challenge/benchmark papers) on topics
    including but not limited to:
  </p>

  <h4>- Micro-Expression Analysis</h4>
  <ul>
    <li>Micro-expression spotting, temporal localization, and onset/offset detection</li>
    <li>Micro-expression recognition, intensity estimation, and fine-grained affect modeling</li>
    <li>Motion representation learning (optical flow, strain, subtle dynamics, action units)</li>
    <li>ME vs. macro-expression / subtle expression discrimination</li>
    <li>Cross-dataset, cross-subject, and cross-cultural generalization</li>
    <li>Robustness to head pose, illumination, occlusion, compression, and low-quality video</li>
  </ul>

  <h4>- Learning with Limited Supervision</h4>
  <ul>
    <li>Self-supervised / unsupervised learning for ME spotting and recognition</li>
    <li>Weakly supervised, semi-supervised, and active learning for ME analysis</li>
    <li>Few-shot, zero-shot, open-set, and continual learning for subtle expressions</li>
    <li>Learning from noisy labels, annotator disagreement, and label uncertainty modeling</li>
    <li>Domain adaptation, test-time adaptation, and personalization</li>
  </ul>

  <h4>- Multimodal, Multi-View, and 3D</h4>
  <ul>
    <li>Multimodal fusion: RGB + NIR/IR/thermal/depth/event cameras</li>
    <li>Physiological signals for affect inference (e.g., ECG/PPG heart rate, EEG, EMG)</li>
    <li>Multi-view facial analysis and 3D/4D face modeling for subtle motion</li>
    <li>Temporal synchronization and alignment across modalities</li>
    <li>Multimodal benchmarks and evaluation protocols</li>
  </ul>

  <h4>- Foundation Models and Vision-Language Approaches</h4>
  <ul>
    <li>Vision-language models (VLMs) for expression understanding and ME reasoning</li>
    <li>Multimodal LLMs for ME analysis with natural-language interaction</li>
    <li>Prompt engineering, instruction tuning, and structured prompting for subtle cues</li>
    <li>Fine-tuning / adapter-based tuning on facial expression corpora for ME sensitivity</li>
    <li>Zero-shot / in-context learning for ME spotting and recognition</li>
    <li>Model interpretability: ensuring genuine ME understanding vs. superficial correlations</li>
  </ul>

  <h4>- Data, Benchmarks, and Reproducibility</h4>
  <ul>
    <li>New spontaneous ME datasets, annotations, and collection protocols</li>
    <li>Benchmark challenges: metrics, splits, standardized evaluation, reproducible baselines</li>
    <li>Synthetic data, simulation, augmentation, and data-centric ME learning</li>
    <li>Bias, fairness, and demographic robustness analysis</li>
    <li>Uncertainty-aware evaluation and confidence calibration</li>
  </ul>

  <h4>- Applications and Responsible Use</h4>
  <ul>
    <li>Affective computing, social signal processing, and human-centered AI</li>
    <li>Mental state and wellbeing analysis (with appropriate ethical safeguards)</li>
    <li>Humanâ€“computer interaction and assistive technologies</li>
    <li>Ethical considerations, privacy, and responsible deployment of ME technologies</li>
  </ul>

  <p>
    <em>Note:</em> The detailed workshop agenda will be announced after paper decisions and final scheduling.
  </p>
</section>

<section>
    <h2 class="major"><span>Submission</span></h2>
    <a name="submission"></a>
    <p>We invite submissions of papers related to facial micro-expression analysis, with a focus on computational methods, multimodal integration, and challenges with large models. The important submission deadlines are as follows:</p>
    <ul>
        <li><strong>Paper Submission Open:</strong> February 16, 2026</li>
        <li><strong>Paper Deadline:</strong> March 16, 2026</li>
        <li><strong>Paper Notification:</strong> April 13, 2026</li>
        <li><strong>Camera Ready:</strong> April 21, 2026 (aligned with the FG conference)</li>
        <li><strong>Workshop Date:</strong> May 25 or 29, 2026 (depending on the schedule)</li>
    </ul>
</section>


            </div>
        </section>

        <footer id="footer">
            <!-- Copyright -->
            <div id="copyright">
                <ul class="menu">
                    <li>GET IN TOUCH: a.davison@mmu.ac.uk | x.fan@mmu.ac.uk | j.see@hw.ac.uk | lijt@psych.ac.cn </li>
                    <li>&copy; MEGC and FME Workshop 2026. All rights reserved</li>
                    <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                </ul>
            </div>
        </footer>

    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.dropotron.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>

</body>

</html>